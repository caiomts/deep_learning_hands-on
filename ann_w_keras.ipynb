{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cc681797-8c05-43a2-849b-ded655108520",
   "metadata": {},
   "source": [
    "# Intro to Artificial Neural Networks with Keras\n",
    "\n",
    "ANNs are the core of **Deep Learning**\n",
    "\n",
    "### Why this wave of interest in ANN's is unlike to die out like died the 1960s and 1980s\n",
    "* ANN's frequently outperform other ML techniques on very large and complex problems;\n",
    "* The increase in computer power since 1990s and cloud platforms have made training large neural networks accessible;\n",
    "* The training algorithms have been improved since 1990s;\n",
    "* ANNs seem to have entered a virtuous circle of funding and progress, as new products based on ANNs are launched more attention towards them are pulled.\n",
    "\n",
    "## Logical Computations with Neurons\n",
    "\n",
    "A simple model of a artificial neuron has on or more binary inputs and one binary output. The AN activates its output when more than a certain number of its inputs are active.\n",
    "\n",
    "*Assumption: a neuron is activated when at least two inputs are active*\n",
    "\n",
    "### Identity function\n",
    "$C = A$\n",
    "\n",
    "$A \\Rightarrow C$\n",
    "\n",
    "*if* A is activated *then* C is activated as well (since it receives two inputs signal)\n",
    "\n",
    "### AND\n",
    "$C = A \\land B$\n",
    "\n",
    "$A \\rightarrow C \\leftarrow B$\n",
    "\n",
    "Neuron C is activated *if and only if* both A *and* B are activated.\n",
    "\n",
    "### OR\n",
    "$C = A \\lor B$\n",
    "\n",
    "$A \\Rightarrow C \\Leftarrow B$\n",
    "\n",
    "Neuron C gets activated *if at least* neuron A *or* B is activated.\n",
    "\n",
    "### When a input connection can inhibit the neuron's activity\n",
    "$C = A \\land \\neg B$\n",
    "\n",
    "$A \\Rightarrow C \\leftarrow \\neg B$\n",
    "\n",
    "Neuron C is activated *only if* A is activated *and* B is deactivated.\n",
    "\n",
    "## The Perceptron\n",
    "One of the simplest ANN architectures and it is based on a slightly different artificial neuron called *threshold logic unit* (TLU) or *linear threshold unit* (LTU). The inputs and outputs are numbers (instead of binary) and each input is associated with a weight. The TLU computes a weighted sum of its inputs\n",
    "$$z = w_1x_1+w_2x_2+\\cdots+w_nx_n = \\mathbf{X}^{\\top}\\mathbf{W}$$\n",
    "then applies a step function to that sum and outputs the result\n",
    "$$h_{\\mathbf{W}}(\\mathbf{X}) = step(z)$$\n",
    "\n",
    "Most common step function used in Perceptrons\n",
    "\n",
    "$$ Heaviside (z) =\n",
    "  \\begin{cases}\n",
    "    0       & \\quad \\text{if } z < t\\\\\n",
    "    1  & \\quad \\text{if } z \\geq t\n",
    "  \\end{cases}\n",
    "$$\n",
    "\n",
    "\n",
    "$$\n",
    "sgn(z)=\n",
    "\\begin{cases}\n",
    "-1 & \\quad \\text{if} z < t\\\\\n",
    "0 & \\quad \\text{if} z = t\\\\\n",
    "+1 &\\quad \\text{if} z> t\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "\n",
    "$$\n",
    "\\text{t: threshold}\n",
    "$$\n",
    "\n",
    "A single TLU would be used for simple linear classification like Logistic Regression or SVM classifier. Training a TLU in this case means finding the right values for $\\mathbf{W}$\n",
    "\n",
    "### Composition\n",
    "\n",
    "A **Perceptron** is composed of a single layer of TLUs with each TLU connected to all inputs (when all neurons in a layer are connected to every single in the previous layer, the layer is called a *fully connected layer* or *dense layer*)\n",
    "\n",
    "The inputs of the Perceptron are fed to special passthrough neurons called input neurons: they output whatever input they are fed. In addition, an extra bias feature is generelly added ($x_0=1$), it's represented using a neuron called *bias neuron*, which outputs 1 all the time.\n",
    "\n",
    "$$h_{\\mathbf{W, b}}=\\phi(\\mathbf{XW}+b)$$\n",
    "Where:  \n",
    "$\\mathbf{X}$: matrix($m\\times n$) of input features.  \n",
    "$\\mathbf{W}$: matrix($n\\times j$) of connection weights one column ($j$) per artificial neuron in the layer.  \n",
    "$\\mathbf{b}$: bias terms vector ($j$) contains all the connection weights between the bias neuron and the artificial neurons. It has one bias term per artificial neuron.$\n",
    "\n",
    "The function $\\phi$ is called activation function\n",
    "\n",
    "### How is a Perceptron trained?\n",
    "Hebb's rule: The connection weight between two neurons tends to increase when they fire simultaneously\n",
    "\n",
    "A variant of the rule takes into account the error made by the network when making a prediction. **The Perceptron learning rule reinforces connections that help reduce the error**.\n",
    "\n",
    "$$W_{i, j}^{\\text{next step}}=W_{i, j}+\\eta(y_j-\\hat{y}_j)x_i$$\n",
    "\n",
    "Where:  \n",
    "$w_{i, j}$ is the connection weight between the $i^{th}$ input neuron and the $j^{th}$ output neuron. \n",
    "$x_i$ is the $i^{th}$ input value of the current training instance.  \n",
    "$\\hat{y}_j$ is the output of the $j^{th}$ output neuron for the current training instance.  \n",
    "$y_j$ is the target output of the $j^{th}$ output neuron for the current training instance.  \n",
    "$\\eta$ is the learning rate.  \n",
    "\n",
    "The decision boundary of each output neuron is linear, so Perceptron are incapable of learning complex patterns. However, if the training instances are linearly separables the algorithm would converge to a solution (*Perceptron convergence theorem*)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "989d20a8-1ec2-4078-abed-eb34e5c444e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)\n",
    "\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8c59467b-388a-4fe6-9063-831d937fe5a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.linear_model import Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9fb27159-74d0-4ae8-8542-0806bf2a37d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = load_iris()\n",
    "X = iris.data[:, (2, 3)] # petal length and petal width\n",
    "y = (iris.target == 0).astype('int')\n",
    "\n",
    "per_clf = Perceptron()\n",
    "per_clf.fit(X, y)\n",
    "\n",
    "y_pred = per_clf.predict([[2, 0.5]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e850021-26d3-4f53-b716-c4c994e4a7c8",
   "metadata": {},
   "source": [
    "`Perceptron` in scikit-learn is equivalent to using an `SGDClassifier` with the fallowing hyperparameters:  \n",
    "`loss='perceptron'`  \n",
    "`learning_rate='constant'`  \n",
    "`eta0='1'`  \n",
    "`penalty=None`  \n",
    "\n",
    "*Contrary to Logistic Regression classifier, Perceptrons do not output a class probability, rather they make predictions based on hard threshold. This is one reason to **prefer** Logistic Regression over Perceptrons*\n",
    "\n",
    "**Perceptron are incabable of solving some trivial problems like *Exclusive OR (XOR)* classification problem. However some of the limitations of perceptrons can be solved by stacking multiple Perceptrons (called Multilayer Perceptron (MLP)).\n",
    "\n",
    "## The Multilayer Perceptron and Backpropagation\n",
    "An MLP is composed of one input layer, one or more layers of TLUs (hidden layers) plus a final TLUs' layer called the output layer.\n",
    "\n",
    "The layers close the input are called *lower layers* and those close to the output *upper layers*. Every layer except the output one includes a bias neuron and is fully connected to the next layer.\n",
    "\n",
    "**Note**: The signal flows only in one direction (from the inputs to outputs), this architecture is an example of *feedforward neural network (FNN)*.\n",
    "\n",
    "**The backpropagation** training algorithm in short is a Gradient Descent using an efficient technique for computing the gradients automatically. In just two pass through the (one forward and one backward), the backpropagation algorithm is able to compute the gradient of the network's error with regard to every single model parameter. In other words, it can find out how each connection weight and each bias term should be tweaked in order to reduce the error. Once it has there gradients it just performs a regular gradient descent step, and the whole process is repeated until the network converge to the solution.\n",
    "\n",
    "**Note**: Automatically computing gradients is called *automatic differentiation*, or *autodiff*. There are various techniques. the one used by backpropagation is called *reverse-mode autodiff*\n",
    "\n",
    "### The algorithm\n",
    "* handles one mini-batch at a time. It goes through the training set multiple times (**Epochs**).\n",
    "* The weights must be randomly initiated.\n",
    "* The algorithm computes the output of all neurons in each layer until the last layer (**forward pass**) and all intermediates results are preserved.\n",
    "* The algorithm computes the network's output error (using a loss function).\n",
    "* Compute how much each output connection contributed to the error (chain rule) and how much of these error contributions come from each connection in the layer below and so on until reaches the input layer. This measures the error gradient across all connection weights in the network by propagating the error backward (**backward pass**).\n",
    "* Finally the algorithm performs a Gradient Descent step to tweak all connection weights in the network using error gradient computed.\n",
    "\n",
    "**Gradiant Descent needs a well-defined non-zero derivative function to make progress at every step. Initially  this function was the sigmoid function**\n",
    "$$\\sigma(z)=\\frac{1}{1+e^{-z}}$$\n",
    "**Other choices:**\n",
    "$$tanh(z)=2\\sigma(2z)-1$$\n",
    "Unlike the sigmoid its output range from $-1$ to $1$ (instead of $0$ to $1$), and the range tends to make each layer's output centered around $0$ at the beginning of training speeding up convergence.\n",
    "$$ReLU(z)=max(0,z)$$\n",
    "Not differentiable at $z=0$ and the derivative is $0$ for $z<0$, but in practice it works well and is fast to compute (has become the default).\n",
    "\n",
    "**A large enough DNN with nonlinear activations can theoretically approximate any continuous function**\n",
    "\n",
    "## Regression MLPs\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
