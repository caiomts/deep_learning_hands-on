{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cc681797-8c05-43a2-849b-ded655108520",
   "metadata": {},
   "source": [
    "# Intro to Artificial Neural Networks with Keras\n",
    "\n",
    "ANNs are the core of **Deep Learning**\n",
    "\n",
    "### Why this wave of interest in ANN's is unlike to die out like died the 1960s and 1980s\n",
    "* ANN's frequently outperform other ML techniques on very large and complex problems;\n",
    "* The increase in computer power since 1990s and cloud platforms have made training large neural networks accessible;\n",
    "* The training algorithms have been improved since 1990s;\n",
    "* ANNs seem to have entered a virtuous circle of funding and progress, as new products based on ANNs are launched more attention towards them are pulled.\n",
    "\n",
    "## Logical Computations with Neurons\n",
    "\n",
    "A simple model of a artificial neuron has on or more binary inputs and one binary output. The AN activates its output when more than a certain number of its inputs are active.\n",
    "\n",
    "*Assumption: a neuron is activated when at least two inputs are active*\n",
    "\n",
    "### Identity function\n",
    "$C = A$\n",
    "\n",
    "$A \\Rightarrow C$\n",
    "\n",
    "*if* A is activated *then* C is activated as well (since it receives two inputs signal)\n",
    "\n",
    "### AND\n",
    "$C = A \\land B$\n",
    "\n",
    "$A \\rightarrow C \\leftarrow B$\n",
    "\n",
    "Neuron C is activated *if and only if* both A *and* B are activated.\n",
    "\n",
    "### OR\n",
    "$C = A \\lor B$\n",
    "\n",
    "$A \\Rightarrow C \\Leftarrow B$\n",
    "\n",
    "Neuron C gets activated *if at least* neuron A *or* B is activated.\n",
    "\n",
    "### When a input connection can inhibit the neuron's activity\n",
    "$C = A \\land \\neg B$\n",
    "\n",
    "$A \\Rightarrow C \\leftarrow \\neg B$\n",
    "\n",
    "Neuron C is activated *only if* A is activated *and* B is deactivated.\n",
    "\n",
    "## The Perceptron\n",
    "One of the simplest ANN architectures and it is based on a slightly different artificial neuron called *threshold logic unit* (TLU) or *linear threshold unit* (LTU). The inputs and outputs are numbers (instead of binary) and each input is associated with a weight. The TLU computes a weighted sum of its inputs\n",
    "$$z = w_1x_1+w_2x_2+\\cdots+w_nx_n = \\mathbf{X}^{\\top}\\mathbf{W}$$\n",
    "then applies a step function to that sum and outputs the result\n",
    "$$h_{\\mathbf{W}}(\\mathbf{X}) = step(z)$$\n",
    "\n",
    "Most common step function used in Perceptrons\n",
    "\n",
    "$$ Heaviside (z) =\n",
    "  \\begin{cases}\n",
    "    0       & \\quad \\text{if } z < t\\\\\n",
    "    1  & \\quad \\text{if } z \\geq t\n",
    "  \\end{cases}\n",
    "$$\n",
    "\n",
    "\n",
    "$$\n",
    "sgn(z)=\n",
    "\\begin{cases}\n",
    "-1 & \\quad \\text{if} z < t\\\\\n",
    "0 & \\quad \\text{if} z = t\\\\\n",
    "+1 &\\quad \\text{if} z> t\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "\n",
    "$$\n",
    "\\text{t: threshold}\n",
    "$$\n",
    "\n",
    "A single TLU would be used for simple linear classification like Logistic Regression or SVM classifier. Training a TLU in this case means finding the right values for $\\mathbf{W}$\n",
    "\n",
    "### Composition\n",
    "\n",
    "A **Perceptron** is composed of a single layer of TLUs with each TLU connected to all inputs (when all neurons in a layer are connected to every single in the previous layer, the layer is called a *fully connected layer* or *dense layer*)\n",
    "\n",
    "The inputs of the Perceptron are fed to special passthrough neurons called input neurons: they output whatever input they are fed. In addition, an extra bias feature is generelly added ($x_0=1$), it's represented using a neuron called *bias neuron*, which outputs 1 all the time.\n",
    "\n",
    "$$h_{\\mathbf{W, b}}=\\phi(\\mathbf{XW}+b)$$\n",
    "Where:  \n",
    "$\\mathbf{X}$: matrix($m\\times n$) of input features.  \n",
    "$\\mathbf{W}$: matrix($n\\times j$) of connection weights one column ($j$) per artificial neuron in the layer.  \n",
    "$\\mathbf{b}$: bias terms vector ($j$) contains all the connection weights between the bias neuron and the artificial neurons. It has one bias term per artificial neuron.$\n",
    "\n",
    "The function $\\phi$ is called activation function\n",
    "\n",
    "### How is a Perceptron trained?\n",
    "Hebb's rule: The connection weight between two neurons tends to increase when they fire simultaneously\n",
    "\n",
    "A variant of the rule takes into account the error made by the network when making a prediction. **The Perceptron learning rule reinforces connections that help reduce the error**.\n",
    "\n",
    "$$W_{i, j}^{\\text{next step}}=W_{i, j}+\\eta(y_j-\\hat{y}_j)x_i$$\n",
    "\n",
    "Where:  \n",
    "$w_{i, j}$ is the connection weight between the $i^{th}$ input neuron and the $j^{th}$ output neuron. \n",
    "$x_i$ is the $i^{th}$ input value of the current training instance.  \n",
    "$\\hat{y}_j$ is the output of the $j^{th}$ output neuron for the current training instance.  \n",
    "$y_j$ is the target output of the $j^{th}$ output neuron for the current training instance.  \n",
    "$\\eta$ is the learning rate.  \n",
    "\n",
    "The decision boundary of each output neuron is linear, so Perceptron are incapable of learning complex patterns. However, if the training instances are linearly separables the algorithm would converge to a solution (*Perceptron convergence theorem*)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "989d20a8-1ec2-4078-abed-eb34e5c444e3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
